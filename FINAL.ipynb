{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"FINAL.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-bMCgOAshnyt","executionInfo":{"status":"ok","timestamp":1614008163241,"user_tz":-480,"elapsed":912,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["import torch \n","from torch import nn\n","from torchvision import transforms,models\n","from torchvision.datasets import ImageFolder\n","from collections import namedtuple\n","from PIL import Image"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7Ej3SZ40VN_","executionInfo":{"status":"ok","timestamp":1614008163242,"user_tz":-480,"elapsed":896,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["dev = torch.device('cuda:0')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z9qPhjb_hpt-","executionInfo":{"status":"ok","timestamp":1614008163243,"user_tz":-480,"elapsed":883,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}},"outputId":"8e2ba3f0-6084-42a0-edba-5de3dc6e7d6e"},"source":["from google.colab import drive \n","drive.mount('/content/drive')\n","\n","from ResNet import ResNet \n","from loss_network import vgg"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uGyK6ZLjhnyz","executionInfo":{"status":"ok","timestamp":1614008163504,"user_tz":-480,"elapsed":1134,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["class Model(object):\n","\n","    def __init__(self):\n","        self.dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        self.transform_net = ResNet().to(self.dev)\n","        self.loss_net = vgg().to(self.dev)\n","        self.loss_net.eval()\n","\n","\n","    def load_dataset(self, dataset_path, style_img_path, image_size=256, batch_size=5):\n","\n","        self.batch_size = batch_size\n","        t = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.CenterCrop(image_size),\n","            transforms.ToTensor(),\n","            transforms.Lambda(lambda x : x.mul(255))\n","        ])\n","\n","        data_folder = ImageFolder(dataset_path, transform=t)\n","        self.loader = torch.utils.data.DataLoader(data_folder, batch_size=batch_size, shuffle=True)\n","\n","        style_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Lambda(lambda x: x.mul(255))\n","        ])\n","\n","        style = Image.open(style_img_path).convert('RGB').resize((image_size, image_size), Image.ANTIALIAS)\n","        self.style_img = style_transform(style).to(self.dev)\n","\n","\n","    def vgg_normalize(self, x):\n","        normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                         std=(0.229, 0.224, 0.225))\n","        x = x.div_(255.0)\n","        out = normalize(x)\n","        return out\n","\n","\n","    def gram_matrix(self, x):\n","        (batch, ch, h, w) = x.size() \n","        img = x.view(batch, ch, h*w)\n","        img_transposed = img.transpose(1,2)\n","        gram_matrix = img.bmm(img_transposed) / (ch * h * w)\n","        return gram_matrix\n","\n","\n","    def load_model(self, model_path):\n","        if model_path != None : \n","            self.transform_net.load_state_dict(torch.load(model_path, map_location=self.dev))\n","\n","\n","    def train(self, epochs, style_weight, content_weight, lr, save_path=None, model_path=None, show_every=20, save_every=100):\n","\n","        self.mseloss = torch.nn.MSELoss()\n","        self.opt = torch.optim.Adam(self.transform_net.parameters(), lr)\n","        self.load_model(model_path)\n","\n","        self.style_img = self.style_img.repeat(self.batch_size, 1, 1, 1)\n","        style_activations = self.loss_net(self.vgg_normalize(self.style_img))\n","        style_grams = [self.gram_matrix(y) for y in style_activations]\n","\n","        for epoch in range(epochs):\n","            self.transform_net.train()\n","            print('EPOCH : ', epoch)\n","\n","            for batch_id, (img, _) in enumerate(self.loader):\n","                n_batch = len(img)\n","                \n","                img = img.to(self.dev)\n","                yhat = self.transform_net(img)\n","\n","                yhat_normalized = self.vgg_normalize(yhat)\n","                content_normalized = self.vgg_normalize(img)\n","\n","                yhat_activations = self.loss_net(yhat_normalized)\n","                content_activations = self.loss_net(content_normalized)\n","\n","                content_loss = content_weight * self.mseloss(yhat_activations.relu2_2, content_activations.relu2_2)\n","\n","                style_loss = 0\n","                for yhat, style_gram in zip(yhat_activations, style_grams):\n","                    yhat_gram = self.gram_matrix(yhat)\n","                    style_loss += self.mseloss(yhat_gram, style_gram[:n_batch , : , :])\n","                style_loss *= style_weight\n","\n","                total_loss = style_loss + content_loss\n","\n","                self.opt.zero_grad()\n","                total_loss.backward(retain_graph=True)\n","                self.opt.step()\n","\n","                if batch_id % show_every == 0 : \n","                    print(f'Batch : {batch_id} | Content loss : {content_loss.item()} | Style loss : {style_loss.item()} | Total loss : {total_loss.item()}')\n","\n","                if batch_id % save_every == 0 : \n","                    if save_path != None : \n","                        torch.save(self.transform_net.state_dict(), save_path)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DfnFz6Yshnyz","executionInfo":{"status":"error","timestamp":1614009687556,"user_tz":-480,"elapsed":1525177,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}},"outputId":"e82855eb-8af2-4822-ad56-9056662a013a"},"source":["dataset_path = '/content/drive/MyDrive/EE/coco'\n","style_img_path = '/content/drive/MyDrive/EE/styles/ooo.jpg'\n","EPOCHS = 10 \n","CONTENT_WEIGHT = 1e5\n","STYLE_WEIGHT = 8e8\n","LR = 0.001\n","SAVE_PATH = '/content/drive/MyDrive/EE/model2.pt'\n","MODEL_PATH = None\n","\n","model = Model()\n","model.load_dataset(dataset_path=dataset_path, style_img_path=style_img_path)\n","model.train(model_path=MODEL_PATH, epochs=EPOCHS, style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT, lr=LR, save_path=SAVE_PATH, show_every=10)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["EPOCH :  0\n","Batch : 0 | Content loss : 603722.875 | Style loss : 3953227.75 | Total loss : 4556950.5\n","Batch : 10 | Content loss : 753697.9375 | Style loss : 3333485.5 | Total loss : 4087183.5\n","Batch : 20 | Content loss : 916206.375 | Style loss : 2678833.5 | Total loss : 3595040.0\n","Batch : 30 | Content loss : 1129492.0 | Style loss : 2193637.0 | Total loss : 3323129.0\n","Batch : 40 | Content loss : 1055943.25 | Style loss : 1845612.375 | Total loss : 2901555.5\n","Batch : 50 | Content loss : 1234030.875 | Style loss : 1455708.75 | Total loss : 2689739.5\n","Batch : 60 | Content loss : 1169994.125 | Style loss : 1316967.0 | Total loss : 2486961.0\n","Batch : 70 | Content loss : 1203522.75 | Style loss : 1135893.75 | Total loss : 2339416.5\n","Batch : 80 | Content loss : 1171420.5 | Style loss : 1007041.0625 | Total loss : 2178461.5\n","Batch : 90 | Content loss : 1121440.375 | Style loss : 971819.1875 | Total loss : 2093259.5\n","Batch : 100 | Content loss : 1268131.0 | Style loss : 836572.875 | Total loss : 2104704.0\n","Batch : 110 | Content loss : 1202716.0 | Style loss : 793231.75 | Total loss : 1995947.75\n","Batch : 120 | Content loss : 1205611.125 | Style loss : 775444.8125 | Total loss : 1981056.0\n","Batch : 130 | Content loss : 1062725.375 | Style loss : 798079.625 | Total loss : 1860805.0\n","Batch : 140 | Content loss : 1177005.375 | Style loss : 718096.375 | Total loss : 1895101.75\n","Batch : 150 | Content loss : 1142817.375 | Style loss : 661069.875 | Total loss : 1803887.25\n","Batch : 160 | Content loss : 1073672.75 | Style loss : 669262.375 | Total loss : 1742935.125\n","Batch : 170 | Content loss : 1224474.625 | Style loss : 593412.5 | Total loss : 1817887.125\n","Batch : 180 | Content loss : 1141466.375 | Style loss : 580170.4375 | Total loss : 1721636.75\n","Batch : 190 | Content loss : 1178166.5 | Style loss : 578287.9375 | Total loss : 1756454.5\n","Batch : 200 | Content loss : 1096444.75 | Style loss : 522350.90625 | Total loss : 1618795.625\n","Batch : 210 | Content loss : 1098001.75 | Style loss : 540369.625 | Total loss : 1638371.375\n","Batch : 220 | Content loss : 1060428.5 | Style loss : 538390.4375 | Total loss : 1598819.0\n","Batch : 230 | Content loss : 1242970.5 | Style loss : 485529.96875 | Total loss : 1728500.5\n","Batch : 240 | Content loss : 1200575.5 | Style loss : 507454.1875 | Total loss : 1708029.75\n","Batch : 250 | Content loss : 1137341.625 | Style loss : 473723.96875 | Total loss : 1611065.625\n","Batch : 260 | Content loss : 1095269.5 | Style loss : 459202.46875 | Total loss : 1554472.0\n","Batch : 270 | Content loss : 1184760.0 | Style loss : 457271.5625 | Total loss : 1642031.5\n","Batch : 280 | Content loss : 1066111.625 | Style loss : 480088.875 | Total loss : 1546200.5\n","Batch : 290 | Content loss : 1192530.5 | Style loss : 470569.125 | Total loss : 1663099.625\n","Batch : 300 | Content loss : 1083871.25 | Style loss : 464943.53125 | Total loss : 1548814.75\n","Batch : 310 | Content loss : 1105675.75 | Style loss : 408452.5625 | Total loss : 1514128.25\n","Batch : 320 | Content loss : 1121595.625 | Style loss : 454817.46875 | Total loss : 1576413.125\n","Batch : 330 | Content loss : 1118918.75 | Style loss : 434186.40625 | Total loss : 1553105.125\n","Batch : 340 | Content loss : 1180129.875 | Style loss : 395022.34375 | Total loss : 1575152.25\n","Batch : 350 | Content loss : 1105870.25 | Style loss : 403491.5625 | Total loss : 1509361.75\n","Batch : 360 | Content loss : 1081603.5 | Style loss : 447689.9375 | Total loss : 1529293.5\n","Batch : 370 | Content loss : 1156475.375 | Style loss : 404370.71875 | Total loss : 1560846.125\n","Batch : 380 | Content loss : 1051458.75 | Style loss : 439986.6875 | Total loss : 1491445.5\n","Batch : 390 | Content loss : 1045752.25 | Style loss : 452241.8125 | Total loss : 1497994.0\n","Batch : 400 | Content loss : 1090802.5 | Style loss : 410952.21875 | Total loss : 1501754.75\n","Batch : 410 | Content loss : 1115206.625 | Style loss : 365319.46875 | Total loss : 1480526.125\n","Batch : 420 | Content loss : 1098835.75 | Style loss : 382888.84375 | Total loss : 1481724.625\n","Batch : 430 | Content loss : 1047355.9375 | Style loss : 427902.40625 | Total loss : 1475258.375\n","Batch : 440 | Content loss : 1163519.75 | Style loss : 398057.78125 | Total loss : 1561577.5\n","Batch : 450 | Content loss : 1098935.0 | Style loss : 446545.25 | Total loss : 1545480.25\n","Batch : 460 | Content loss : 1039996.625 | Style loss : 394130.1875 | Total loss : 1434126.75\n","Batch : 470 | Content loss : 1163198.375 | Style loss : 385317.6875 | Total loss : 1548516.0\n","Batch : 480 | Content loss : 1129675.625 | Style loss : 365600.65625 | Total loss : 1495276.25\n","Batch : 490 | Content loss : 1057800.875 | Style loss : 353542.75 | Total loss : 1411343.625\n","Batch : 500 | Content loss : 1037357.3125 | Style loss : 345350.46875 | Total loss : 1382707.75\n","Batch : 510 | Content loss : 1005279.4375 | Style loss : 372523.5 | Total loss : 1377803.0\n","Batch : 520 | Content loss : 1048978.125 | Style loss : 382991.3125 | Total loss : 1431969.5\n","Batch : 530 | Content loss : 1050828.75 | Style loss : 347599.84375 | Total loss : 1398428.625\n","Batch : 540 | Content loss : 1025026.5 | Style loss : 357555.15625 | Total loss : 1382581.625\n","Batch : 550 | Content loss : 1051496.375 | Style loss : 418305.21875 | Total loss : 1469801.625\n","Batch : 560 | Content loss : 988077.5625 | Style loss : 410253.9375 | Total loss : 1398331.5\n","Batch : 570 | Content loss : 1042328.375 | Style loss : 370026.5 | Total loss : 1412354.875\n","Batch : 580 | Content loss : 1017223.375 | Style loss : 368119.5 | Total loss : 1385342.875\n","Batch : 590 | Content loss : 1100756.375 | Style loss : 345728.9375 | Total loss : 1446485.25\n","Batch : 600 | Content loss : 1016521.375 | Style loss : 360624.875 | Total loss : 1377146.25\n","Batch : 610 | Content loss : 1043338.5 | Style loss : 335650.65625 | Total loss : 1378989.125\n","Batch : 620 | Content loss : 994010.375 | Style loss : 358285.4375 | Total loss : 1352295.75\n","Batch : 630 | Content loss : 973752.875 | Style loss : 351395.65625 | Total loss : 1325148.5\n","Batch : 640 | Content loss : 1004488.5625 | Style loss : 340434.0625 | Total loss : 1344922.625\n","Batch : 650 | Content loss : 994103.6875 | Style loss : 352519.0 | Total loss : 1346622.75\n","Batch : 660 | Content loss : 1010921.375 | Style loss : 357342.59375 | Total loss : 1368264.0\n","Batch : 670 | Content loss : 1036799.125 | Style loss : 345235.96875 | Total loss : 1382035.125\n","Batch : 680 | Content loss : 1014164.8125 | Style loss : 315328.6875 | Total loss : 1329493.5\n","Batch : 690 | Content loss : 1005573.4375 | Style loss : 309413.9375 | Total loss : 1314987.375\n","Batch : 700 | Content loss : 1015986.375 | Style loss : 318771.28125 | Total loss : 1334757.625\n","Batch : 710 | Content loss : 1004333.375 | Style loss : 323703.40625 | Total loss : 1328036.75\n","Batch : 720 | Content loss : 1002143.3125 | Style loss : 315950.96875 | Total loss : 1318094.25\n","Batch : 730 | Content loss : 1018601.125 | Style loss : 294011.65625 | Total loss : 1312612.75\n","Batch : 740 | Content loss : 1048400.0 | Style loss : 310041.5 | Total loss : 1358441.5\n","Batch : 750 | Content loss : 1004901.125 | Style loss : 301311.59375 | Total loss : 1306212.75\n","Batch : 760 | Content loss : 969057.375 | Style loss : 386503.3125 | Total loss : 1355560.75\n","Batch : 770 | Content loss : 988669.375 | Style loss : 310299.8125 | Total loss : 1298969.25\n","Batch : 780 | Content loss : 977932.1875 | Style loss : 321160.75 | Total loss : 1299093.0\n","Batch : 790 | Content loss : 962153.8125 | Style loss : 344669.25 | Total loss : 1306823.0\n","Batch : 800 | Content loss : 1015790.1875 | Style loss : 301464.25 | Total loss : 1317254.5\n","Batch : 810 | Content loss : 977657.3125 | Style loss : 298578.84375 | Total loss : 1276236.125\n","Batch : 820 | Content loss : 955906.3125 | Style loss : 351105.6875 | Total loss : 1307012.0\n","Batch : 830 | Content loss : 998313.8125 | Style loss : 307191.875 | Total loss : 1305505.75\n","Batch : 840 | Content loss : 931255.8125 | Style loss : 337616.46875 | Total loss : 1268872.25\n","Batch : 850 | Content loss : 982189.9375 | Style loss : 311042.28125 | Total loss : 1293232.25\n","Batch : 860 | Content loss : 979964.9375 | Style loss : 322789.625 | Total loss : 1302754.5\n","Batch : 870 | Content loss : 945755.6875 | Style loss : 336159.6875 | Total loss : 1281915.375\n","Batch : 880 | Content loss : 939904.6875 | Style loss : 342164.6875 | Total loss : 1282069.375\n","Batch : 890 | Content loss : 982845.1875 | Style loss : 342561.3125 | Total loss : 1325406.5\n","Batch : 900 | Content loss : 1012363.9375 | Style loss : 275171.375 | Total loss : 1287535.25\n","Batch : 910 | Content loss : 987523.625 | Style loss : 315875.21875 | Total loss : 1303398.875\n","Batch : 920 | Content loss : 982624.3125 | Style loss : 356827.1875 | Total loss : 1339451.5\n","Batch : 930 | Content loss : 900248.25 | Style loss : 315245.0 | Total loss : 1215493.25\n","Batch : 940 | Content loss : 973796.75 | Style loss : 326201.4375 | Total loss : 1299998.25\n","Batch : 950 | Content loss : 956917.0 | Style loss : 338328.71875 | Total loss : 1295245.75\n","Batch : 960 | Content loss : 851781.9375 | Style loss : 382707.90625 | Total loss : 1234489.875\n","Batch : 970 | Content loss : 953214.5625 | Style loss : 345233.1875 | Total loss : 1298447.75\n","Batch : 980 | Content loss : 880459.875 | Style loss : 339558.84375 | Total loss : 1220018.75\n","Batch : 990 | Content loss : 948659.5 | Style loss : 294435.6875 | Total loss : 1243095.25\n","Batch : 1000 | Content loss : 957730.0 | Style loss : 312094.75 | Total loss : 1269824.75\n","Batch : 1010 | Content loss : 986281.875 | Style loss : 287145.90625 | Total loss : 1273427.75\n","Batch : 1020 | Content loss : 898870.0 | Style loss : 334718.625 | Total loss : 1233588.625\n","Batch : 1030 | Content loss : 962284.375 | Style loss : 315425.09375 | Total loss : 1277709.5\n","Batch : 1040 | Content loss : 929596.5 | Style loss : 315098.1875 | Total loss : 1244694.75\n","Batch : 1050 | Content loss : 908640.5625 | Style loss : 360858.96875 | Total loss : 1269499.5\n","Batch : 1060 | Content loss : 1050681.625 | Style loss : 283811.0 | Total loss : 1334492.625\n","Batch : 1070 | Content loss : 936614.6875 | Style loss : 265417.40625 | Total loss : 1202032.125\n","Batch : 1080 | Content loss : 892244.25 | Style loss : 296002.03125 | Total loss : 1188246.25\n","Batch : 1090 | Content loss : 952324.9375 | Style loss : 330138.5 | Total loss : 1282463.5\n","Batch : 1100 | Content loss : 931846.0625 | Style loss : 290762.96875 | Total loss : 1222609.0\n","Batch : 1110 | Content loss : 952826.3125 | Style loss : 272226.21875 | Total loss : 1225052.5\n","Batch : 1120 | Content loss : 927681.0625 | Style loss : 315532.71875 | Total loss : 1243213.75\n","Batch : 1130 | Content loss : 901490.8125 | Style loss : 325734.34375 | Total loss : 1227225.125\n","Batch : 1140 | Content loss : 925474.4375 | Style loss : 290303.875 | Total loss : 1215778.25\n","Batch : 1150 | Content loss : 892410.9375 | Style loss : 270146.21875 | Total loss : 1162557.125\n","Batch : 1160 | Content loss : 912706.9375 | Style loss : 275739.1875 | Total loss : 1188446.125\n","Batch : 1170 | Content loss : 939966.3125 | Style loss : 286516.375 | Total loss : 1226482.75\n","Batch : 1180 | Content loss : 915433.125 | Style loss : 306506.5625 | Total loss : 1221939.75\n","Batch : 1190 | Content loss : 901604.75 | Style loss : 309350.3125 | Total loss : 1210955.0\n","Batch : 1200 | Content loss : 888391.75 | Style loss : 316572.4375 | Total loss : 1204964.25\n","Batch : 1210 | Content loss : 886180.125 | Style loss : 311401.78125 | Total loss : 1197581.875\n","Batch : 1220 | Content loss : 898540.125 | Style loss : 334295.34375 | Total loss : 1232835.5\n","Batch : 1230 | Content loss : 912874.625 | Style loss : 334614.3125 | Total loss : 1247489.0\n","Batch : 1240 | Content loss : 873561.9375 | Style loss : 298190.09375 | Total loss : 1171752.0\n","Batch : 1250 | Content loss : 945770.1875 | Style loss : 316629.0625 | Total loss : 1262399.25\n","Batch : 1260 | Content loss : 833878.8125 | Style loss : 299391.1875 | Total loss : 1133270.0\n","Batch : 1270 | Content loss : 961877.9375 | Style loss : 333803.0 | Total loss : 1295681.0\n","Batch : 1280 | Content loss : 891703.25 | Style loss : 350753.46875 | Total loss : 1242456.75\n","Batch : 1290 | Content loss : 932333.25 | Style loss : 288564.84375 | Total loss : 1220898.125\n","Batch : 1300 | Content loss : 831120.125 | Style loss : 346510.375 | Total loss : 1177630.5\n","Batch : 1310 | Content loss : 869244.875 | Style loss : 348338.625 | Total loss : 1217583.5\n","Batch : 1320 | Content loss : 894771.125 | Style loss : 319793.0625 | Total loss : 1214564.25\n","Batch : 1330 | Content loss : 908727.625 | Style loss : 305426.78125 | Total loss : 1214154.375\n","Batch : 1340 | Content loss : 895304.0 | Style loss : 284135.53125 | Total loss : 1179439.5\n","Batch : 1350 | Content loss : 925702.1875 | Style loss : 336504.03125 | Total loss : 1262206.25\n","Batch : 1360 | Content loss : 930151.375 | Style loss : 351992.6875 | Total loss : 1282144.0\n","Batch : 1370 | Content loss : 864701.375 | Style loss : 295017.5 | Total loss : 1159718.875\n","Batch : 1380 | Content loss : 905887.4375 | Style loss : 298537.8125 | Total loss : 1204425.25\n","Batch : 1390 | Content loss : 901209.9375 | Style loss : 295001.46875 | Total loss : 1196211.375\n","Batch : 1400 | Content loss : 903362.1875 | Style loss : 270087.25 | Total loss : 1173449.5\n","Batch : 1410 | Content loss : 812772.25 | Style loss : 279377.0625 | Total loss : 1092149.25\n","Batch : 1420 | Content loss : 863897.4375 | Style loss : 296131.625 | Total loss : 1160029.0\n","Batch : 1430 | Content loss : 892134.5625 | Style loss : 333059.125 | Total loss : 1225193.75\n","Batch : 1440 | Content loss : 891992.3125 | Style loss : 324141.125 | Total loss : 1216133.5\n","Batch : 1450 | Content loss : 862016.3125 | Style loss : 281245.9375 | Total loss : 1143262.25\n","Batch : 1460 | Content loss : 836858.375 | Style loss : 347209.625 | Total loss : 1184068.0\n","Batch : 1470 | Content loss : 905136.875 | Style loss : 326598.84375 | Total loss : 1231735.75\n","Batch : 1480 | Content loss : 846199.25 | Style loss : 290945.1875 | Total loss : 1137144.5\n","Batch : 1490 | Content loss : 888769.0625 | Style loss : 312856.75 | Total loss : 1201625.75\n","Batch : 1500 | Content loss : 870932.3125 | Style loss : 274388.0625 | Total loss : 1145320.375\n","Batch : 1510 | Content loss : 860758.125 | Style loss : 341548.40625 | Total loss : 1202306.5\n","Batch : 1520 | Content loss : 875204.5625 | Style loss : 328560.3125 | Total loss : 1203764.875\n","Batch : 1530 | Content loss : 847220.0625 | Style loss : 260073.765625 | Total loss : 1107293.875\n","Batch : 1540 | Content loss : 941647.25 | Style loss : 280219.375 | Total loss : 1221866.625\n","Batch : 1550 | Content loss : 835584.4375 | Style loss : 319966.625 | Total loss : 1155551.0\n","Batch : 1560 | Content loss : 859286.6875 | Style loss : 306080.03125 | Total loss : 1165366.75\n","Batch : 1570 | Content loss : 885416.8125 | Style loss : 321732.75 | Total loss : 1207149.5\n","Batch : 1580 | Content loss : 845693.0 | Style loss : 349632.75 | Total loss : 1195325.75\n","Batch : 1590 | Content loss : 847552.875 | Style loss : 314875.4375 | Total loss : 1162428.25\n","Batch : 1600 | Content loss : 851773.375 | Style loss : 345932.03125 | Total loss : 1197705.375\n","Batch : 1610 | Content loss : 825758.0 | Style loss : 326281.0 | Total loss : 1152039.0\n","Batch : 1620 | Content loss : 987004.75 | Style loss : 272304.15625 | Total loss : 1259308.875\n","EPOCH :  1\n","Batch : 0 | Content loss : 805129.125 | Style loss : 379796.0625 | Total loss : 1184925.25\n","Batch : 10 | Content loss : 909748.75 | Style loss : 311332.28125 | Total loss : 1221081.0\n","Batch : 20 | Content loss : 862705.5 | Style loss : 291744.09375 | Total loss : 1154449.625\n","Batch : 30 | Content loss : 888412.75 | Style loss : 320868.21875 | Total loss : 1209281.0\n","Batch : 40 | Content loss : 779605.75 | Style loss : 384592.1875 | Total loss : 1164198.0\n","Batch : 50 | Content loss : 875247.875 | Style loss : 319366.8125 | Total loss : 1194614.75\n","Batch : 60 | Content loss : 870680.875 | Style loss : 308621.875 | Total loss : 1179302.75\n","Batch : 70 | Content loss : 874604.3125 | Style loss : 282055.78125 | Total loss : 1156660.125\n","Batch : 80 | Content loss : 810877.125 | Style loss : 295334.375 | Total loss : 1106211.5\n","Batch : 90 | Content loss : 881385.3125 | Style loss : 328322.46875 | Total loss : 1209707.75\n","Batch : 100 | Content loss : 828290.875 | Style loss : 310403.25 | Total loss : 1138694.125\n","Batch : 110 | Content loss : 803654.5 | Style loss : 285356.875 | Total loss : 1089011.375\n","Batch : 120 | Content loss : 889031.3125 | Style loss : 306151.375 | Total loss : 1195182.75\n","Batch : 130 | Content loss : 855985.75 | Style loss : 314838.40625 | Total loss : 1170824.125\n","Batch : 140 | Content loss : 849806.375 | Style loss : 243303.53125 | Total loss : 1093109.875\n","Batch : 150 | Content loss : 832475.1875 | Style loss : 314685.875 | Total loss : 1147161.0\n","Batch : 160 | Content loss : 856769.75 | Style loss : 304515.1875 | Total loss : 1161285.0\n","Batch : 170 | Content loss : 824737.75 | Style loss : 262031.734375 | Total loss : 1086769.5\n","Batch : 180 | Content loss : 859896.75 | Style loss : 277862.03125 | Total loss : 1137758.75\n","Batch : 190 | Content loss : 878290.4375 | Style loss : 354085.0625 | Total loss : 1232375.5\n","Batch : 200 | Content loss : 877341.625 | Style loss : 294770.3125 | Total loss : 1172112.0\n","Batch : 210 | Content loss : 837087.4375 | Style loss : 312032.03125 | Total loss : 1149119.5\n","Batch : 220 | Content loss : 863715.8125 | Style loss : 305846.71875 | Total loss : 1169562.5\n","Batch : 230 | Content loss : 836258.0 | Style loss : 282501.90625 | Total loss : 1118759.875\n","Batch : 240 | Content loss : 861932.375 | Style loss : 286826.78125 | Total loss : 1148759.125\n","Batch : 250 | Content loss : 829386.625 | Style loss : 284015.46875 | Total loss : 1113402.125\n","Batch : 260 | Content loss : 800883.375 | Style loss : 264534.1875 | Total loss : 1065417.5\n","Batch : 270 | Content loss : 838783.0 | Style loss : 318007.4375 | Total loss : 1156790.5\n","Batch : 280 | Content loss : 833885.9375 | Style loss : 283829.875 | Total loss : 1117715.75\n","Batch : 290 | Content loss : 912866.5 | Style loss : 327877.3125 | Total loss : 1240743.75\n","Batch : 300 | Content loss : 819008.75 | Style loss : 292330.6875 | Total loss : 1111339.5\n","Batch : 310 | Content loss : 828930.5625 | Style loss : 276054.90625 | Total loss : 1104985.5\n","Batch : 320 | Content loss : 816897.125 | Style loss : 293784.15625 | Total loss : 1110681.25\n","Batch : 330 | Content loss : 900117.5625 | Style loss : 330788.0 | Total loss : 1230905.5\n","Batch : 340 | Content loss : 842187.8125 | Style loss : 303123.53125 | Total loss : 1145311.375\n","Batch : 350 | Content loss : 815091.125 | Style loss : 309171.21875 | Total loss : 1124262.375\n","Batch : 360 | Content loss : 874652.125 | Style loss : 348448.9375 | Total loss : 1223101.0\n","Batch : 370 | Content loss : 848150.5625 | Style loss : 321264.40625 | Total loss : 1169415.0\n","Batch : 380 | Content loss : 790468.625 | Style loss : 282067.6875 | Total loss : 1072536.25\n","Batch : 390 | Content loss : 860501.9375 | Style loss : 317099.96875 | Total loss : 1177601.875\n","Batch : 400 | Content loss : 878954.5 | Style loss : 287052.4375 | Total loss : 1166007.0\n","Batch : 410 | Content loss : 787559.875 | Style loss : 349029.375 | Total loss : 1136589.25\n","Batch : 420 | Content loss : 864158.8125 | Style loss : 339163.6875 | Total loss : 1203322.5\n","Batch : 430 | Content loss : 838083.5625 | Style loss : 261524.921875 | Total loss : 1099608.5\n","Batch : 440 | Content loss : 810734.4375 | Style loss : 306357.90625 | Total loss : 1117092.375\n","Batch : 450 | Content loss : 821696.375 | Style loss : 306615.4375 | Total loss : 1128311.75\n","Batch : 460 | Content loss : 808708.1875 | Style loss : 262317.53125 | Total loss : 1071025.75\n","Batch : 470 | Content loss : 852711.5625 | Style loss : 322129.90625 | Total loss : 1174841.5\n","Batch : 480 | Content loss : 795385.4375 | Style loss : 306823.375 | Total loss : 1102208.75\n","Batch : 490 | Content loss : 883103.875 | Style loss : 320856.5625 | Total loss : 1203960.5\n","Batch : 500 | Content loss : 843849.875 | Style loss : 330222.84375 | Total loss : 1174072.75\n","Batch : 510 | Content loss : 800746.625 | Style loss : 297621.0 | Total loss : 1098367.625\n","Batch : 520 | Content loss : 789744.1875 | Style loss : 267271.5 | Total loss : 1057015.75\n","Batch : 530 | Content loss : 805020.8125 | Style loss : 323045.78125 | Total loss : 1128066.625\n","Batch : 540 | Content loss : 832916.75 | Style loss : 320514.4375 | Total loss : 1153431.25\n","Batch : 550 | Content loss : 811441.4375 | Style loss : 280827.96875 | Total loss : 1092269.375\n","Batch : 560 | Content loss : 812859.9375 | Style loss : 309231.46875 | Total loss : 1122091.375\n","Batch : 570 | Content loss : 825595.3125 | Style loss : 307027.8125 | Total loss : 1132623.125\n","Batch : 580 | Content loss : 856183.5 | Style loss : 316225.125 | Total loss : 1172408.625\n","Batch : 590 | Content loss : 845081.5 | Style loss : 312935.625 | Total loss : 1158017.125\n","Batch : 600 | Content loss : 810436.125 | Style loss : 285378.34375 | Total loss : 1095814.5\n","Batch : 610 | Content loss : 848933.3125 | Style loss : 326979.3125 | Total loss : 1175912.625\n","Batch : 620 | Content loss : 845070.625 | Style loss : 284513.75 | Total loss : 1129584.375\n","Batch : 630 | Content loss : 759933.625 | Style loss : 308816.5625 | Total loss : 1068750.25\n","Batch : 640 | Content loss : 782395.8125 | Style loss : 308224.25 | Total loss : 1090620.0\n","Batch : 650 | Content loss : 801330.25 | Style loss : 345204.9375 | Total loss : 1146535.25\n","Batch : 660 | Content loss : 828859.4375 | Style loss : 293116.375 | Total loss : 1121975.75\n","Batch : 670 | Content loss : 827276.25 | Style loss : 293510.53125 | Total loss : 1120786.75\n","Batch : 680 | Content loss : 822915.5625 | Style loss : 294987.84375 | Total loss : 1117903.375\n","Batch : 690 | Content loss : 808749.375 | Style loss : 311271.09375 | Total loss : 1120020.5\n","Batch : 700 | Content loss : 861867.875 | Style loss : 299305.53125 | Total loss : 1161173.375\n","Batch : 710 | Content loss : 796837.5 | Style loss : 257844.96875 | Total loss : 1054682.5\n","Batch : 720 | Content loss : 800814.25 | Style loss : 312184.8125 | Total loss : 1112999.0\n","Batch : 730 | Content loss : 792328.1875 | Style loss : 289989.625 | Total loss : 1082317.75\n","Batch : 740 | Content loss : 858307.4375 | Style loss : 311048.96875 | Total loss : 1169356.375\n","Batch : 750 | Content loss : 856597.0625 | Style loss : 279142.625 | Total loss : 1135739.75\n","Batch : 760 | Content loss : 806180.5625 | Style loss : 273478.46875 | Total loss : 1079659.0\n","Batch : 770 | Content loss : 796567.6875 | Style loss : 281036.1875 | Total loss : 1077603.875\n","Batch : 780 | Content loss : 844697.3125 | Style loss : 335589.0 | Total loss : 1180286.25\n","Batch : 790 | Content loss : 846374.4375 | Style loss : 282511.5 | Total loss : 1128886.0\n","Batch : 800 | Content loss : 788207.25 | Style loss : 315269.09375 | Total loss : 1103476.375\n","Batch : 810 | Content loss : 805390.375 | Style loss : 319434.71875 | Total loss : 1124825.125\n","Batch : 820 | Content loss : 802430.625 | Style loss : 313632.125 | Total loss : 1116062.75\n","Batch : 830 | Content loss : 846465.5625 | Style loss : 309299.40625 | Total loss : 1155765.0\n","Batch : 840 | Content loss : 834297.75 | Style loss : 283733.15625 | Total loss : 1118030.875\n","Batch : 850 | Content loss : 814575.6875 | Style loss : 315831.53125 | Total loss : 1130407.25\n","Batch : 860 | Content loss : 804868.8125 | Style loss : 312034.75 | Total loss : 1116903.5\n","Batch : 870 | Content loss : 774505.75 | Style loss : 275980.71875 | Total loss : 1050486.5\n","Batch : 880 | Content loss : 813814.0 | Style loss : 292126.8125 | Total loss : 1105940.75\n","Batch : 890 | Content loss : 783896.75 | Style loss : 306080.75 | Total loss : 1089977.5\n","Batch : 900 | Content loss : 839035.5 | Style loss : 314739.21875 | Total loss : 1153774.75\n","Batch : 910 | Content loss : 777503.8125 | Style loss : 264962.34375 | Total loss : 1042466.125\n","Batch : 920 | Content loss : 801599.4375 | Style loss : 316380.4375 | Total loss : 1117979.875\n","Batch : 930 | Content loss : 820454.1875 | Style loss : 315504.5625 | Total loss : 1135958.75\n","Batch : 940 | Content loss : 821793.75 | Style loss : 312752.75 | Total loss : 1134546.5\n","Batch : 950 | Content loss : 813143.625 | Style loss : 303297.46875 | Total loss : 1116441.125\n","Batch : 960 | Content loss : 772487.375 | Style loss : 304134.0625 | Total loss : 1076621.5\n","Batch : 970 | Content loss : 795303.9375 | Style loss : 258678.734375 | Total loss : 1053982.625\n","Batch : 980 | Content loss : 796525.125 | Style loss : 328763.46875 | Total loss : 1125288.625\n","Batch : 990 | Content loss : 852398.5 | Style loss : 263381.125 | Total loss : 1115779.625\n","Batch : 1000 | Content loss : 794879.9375 | Style loss : 264065.6875 | Total loss : 1058945.625\n","Batch : 1010 | Content loss : 792771.75 | Style loss : 337961.28125 | Total loss : 1130733.0\n","Batch : 1020 | Content loss : 816109.0 | Style loss : 274544.46875 | Total loss : 1090653.5\n","Batch : 1030 | Content loss : 807485.75 | Style loss : 279661.5625 | Total loss : 1087147.25\n","Batch : 1040 | Content loss : 769320.3125 | Style loss : 303359.6875 | Total loss : 1072680.0\n","Batch : 1050 | Content loss : 739374.75 | Style loss : 281941.6875 | Total loss : 1021316.4375\n","Batch : 1060 | Content loss : 854124.0625 | Style loss : 293765.84375 | Total loss : 1147889.875\n","Batch : 1070 | Content loss : 840356.4375 | Style loss : 351508.59375 | Total loss : 1191865.0\n","Batch : 1080 | Content loss : 788429.0 | Style loss : 319551.34375 | Total loss : 1107980.375\n","Batch : 1090 | Content loss : 791956.6875 | Style loss : 324981.75 | Total loss : 1116938.5\n","Batch : 1100 | Content loss : 794642.0 | Style loss : 333971.84375 | Total loss : 1128613.875\n","Batch : 1110 | Content loss : 812711.625 | Style loss : 335157.28125 | Total loss : 1147868.875\n","Batch : 1120 | Content loss : 767566.75 | Style loss : 287241.21875 | Total loss : 1054808.0\n","Batch : 1130 | Content loss : 837522.875 | Style loss : 325812.59375 | Total loss : 1163335.5\n","Batch : 1140 | Content loss : 834098.75 | Style loss : 332386.96875 | Total loss : 1166485.75\n","Batch : 1150 | Content loss : 827585.0 | Style loss : 272489.59375 | Total loss : 1100074.625\n","Batch : 1160 | Content loss : 808211.4375 | Style loss : 316411.40625 | Total loss : 1124622.875\n","Batch : 1170 | Content loss : 846810.4375 | Style loss : 294498.1875 | Total loss : 1141308.625\n","Batch : 1180 | Content loss : 775022.875 | Style loss : 323621.4375 | Total loss : 1098644.25\n","Batch : 1190 | Content loss : 851557.0625 | Style loss : 304494.28125 | Total loss : 1156051.375\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-7043d3fb777e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_img_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTYLE_WEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONTENT_WEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-5d49f437c08e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, style_weight, content_weight, lr, save_path, model_path, show_every, save_every)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EPOCH : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[1;32m    150\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \"\"\"\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}